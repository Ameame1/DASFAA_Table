# æ·±åº¦è°ƒç ”æŠ¥å‘Š:åŸºäºAILS-NTUAçš„GRPOå¢å¼ºå®ç°æ–¹æ¡ˆ

> **è°ƒç ”æ—¥æœŸ**: 2025-01
> **ç›®æ ‡**: ä¸ºTable QAä»»åŠ¡æä¾›è¯¦ç»†çš„æŠ€æœ¯å®ç°è·¯çº¿

---

## ä¸€ã€æ ¸å¿ƒå·¥ä½œæ·±åº¦åˆ†æ

### ğŸ” **1. AILS-NTUA (SemEval 2025å† å†›)**

#### **æ¶æ„åˆ†æ**
```python
# è®ºæ–‡æåˆ°çš„åŒæ¨¡å—æ¶æ„

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Module: Query â†’ Python Code      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input: Table + Question          â”‚  â”‚
â”‚  â”‚ â†“                                â”‚  â”‚
â”‚  â”‚ Step 1: Decompose Task           â”‚  â”‚
â”‚  â”‚   - å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜          â”‚  â”‚
â”‚  â”‚ â†“                                â”‚  â”‚
â”‚  â”‚ Step 2: Generate Python Function â”‚  â”‚
â”‚  â”‚   - ä½¿ç”¨LLM promptingç”Ÿæˆä»£ç       â”‚  â”‚
â”‚  â”‚ â†“                                â”‚  â”‚
â”‚  â”‚ Step 3: Execute Code             â”‚  â”‚
â”‚  â”‚   - åœ¨sandboxä¸­æ‰§è¡Œ                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ (å¦‚æœæ‰§è¡Œå¤±è´¥)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Error-Fixing Module: Code Refinement  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input: Error Message + Old Code  â”‚  â”‚
â”‚  â”‚ â†“                                â”‚  â”‚
â”‚  â”‚ Self-Correction Mechanism        â”‚  â”‚
â”‚  â”‚   - åˆ†æé”™è¯¯                      â”‚  â”‚
â”‚  â”‚   - ç”Ÿæˆä¿®å¤æç¤º                  â”‚  â”‚
â”‚  â”‚   - é‡æ–°ç”Ÿæˆä»£ç                   â”‚  â”‚
â”‚  â”‚ â†“                                â”‚  â”‚
â”‚  â”‚ Re-execute (æœ€å¤š2æ¬¡è¿­ä»£)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **å…³é”®æŠ€æœ¯ç»†èŠ‚**
1. **Task Decomposition**: å°†é—®é¢˜åˆ†è§£ä¸ºå­ä»»åŠ¡
2. **LLM Prompting**: ç²¾å¿ƒè®¾è®¡çš„prompt templates
3. **Self-Correction**: ç®€å•çš„error messageåé¦ˆ
4. **è¿­ä»£ä¸Šé™**: å›ºå®š2æ¬¡

#### **ä»£ç å¯ç”¨æ€§**
- âœ… è®ºæ–‡æåˆ°ä»£ç åœ¨GitHub (ä½†å…·ä½“repoæœªæ‰¾åˆ°)
- âœ… å›¢é˜Ÿæ¥è‡ªNTUA (National Technical University of Athens)
- âœ… å¯ä»¥å°è¯•è”ç³»ä½œè€…: Andreas Evangelatosç­‰

#### **æˆ‘ä»¬çš„æ”¹è¿›ç‚¹**
```
AILS-NTUAçš„å±€é™           â†’  æˆ‘ä»¬çš„æ”¹è¿›
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç®€å•error messageåé¦ˆ      â†’  4å±‚åˆ†å±‚è¯Šæ–­
å›ºå®š2æ¬¡è¿­ä»£                â†’  åŠ¨æ€1-5æ¬¡è¿­ä»£
æ— ç»“æ„åŒ–ä¿®å¤ç­–ç•¥           â†’  20+ä¸“ç”¨ç­–ç•¥åº“
æ— RLä¼˜åŒ–                  â†’  GRPOè®­ç»ƒè¿­ä»£ç­–ç•¥
```

---

### ğŸ” **2. Table-R1 (GRPO for Table QA)**

#### **æ ¸å¿ƒåˆ›æ–°: TARPOç®—æ³•**

```python
# Table-Aware Region Policy Optimization

class TARPO:
    """
    Table-R1çš„æ ¸å¿ƒ:æ‰©å±•GRPOç”¨äºè¡¨æ ¼ç†è§£
    """

    def __init__(self):
        # Stage 1: Region-Enhanced SFT
        self.re_sft = RegionEnhancedSFT()

        # Stage 2: GRPOè®­ç»ƒ
        self.grpo_trainer = GRPOTrainer()

    def train(self, table, question):
        # Step 1: è¯†åˆ«ç›¸å…³è¡¨æ ¼åŒºåŸŸ
        regions = self.identify_regions(table, question)

        # Step 2: åŸºäºåŒºåŸŸç”Ÿæˆæ¨ç†
        reasoning_steps = self.generate_reasoning(table, regions, question)

        # Step 3: è®¡ç®—mixed reward
        reward = self.compute_mixed_reward(
            region_accuracy=regions.accuracy,
            answer_correctness=reasoning_steps.answer_correct,
            consistency=regions.consistency
        )

        return reward

    def compute_mixed_reward(self, region_accuracy, answer_correctness, consistency):
        """
        Mixed Reward System (Table-R1çš„åˆ›æ–°)
        """
        # Decaying region rewards (è®­ç»ƒåæœŸé™ä½regionæƒé‡)
        alpha = self.decay_schedule(epoch)

        reward = (
            alpha * region_accuracy +        # åŒºåŸŸå‡†ç¡®ç‡
            (1 - alpha) * answer_correctness + # ç­”æ¡ˆæ­£ç¡®ç‡
            -0.1 * (1 - consistency)         # ä¸€è‡´æ€§æƒ©ç½š
        )

        return reward
```

#### **GRPOå®ç°ç»†èŠ‚ (åŸºäºDeepSeek)**

```python
class GRPOTrainer:
    """
    Group Relative Policy Optimization
    æ ¸å¿ƒ:ç”¨group meanä½œä¸ºbaseline,ä¸éœ€è¦critic network
    """

    def __init__(self, group_size=4):
        self.group_size = group_size

    def train_step(self, prompts, policy_model, ref_model):
        # Step 1: å¯¹æ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªresponses (group_sizeä¸ª)
        responses = []
        for prompt in prompts:
            group_responses = []
            for _ in range(self.group_size):
                response = policy_model.generate(prompt)
                group_responses.append(response)
            responses.append(group_responses)

        # Step 2: è®¡ç®—æ¯ä¸ªresponseçš„reward
        rewards = []
        for group in responses:
            group_rewards = [compute_reward(r) for r in group]
            rewards.append(group_rewards)

        # Step 3: Group-based advantage estimation
        advantages = []
        for group_rewards in rewards:
            group_mean = np.mean(group_rewards)
            group_std = np.std(group_rewards) + 1e-8

            # å…³é”®:ç”¨group meanä½œä¸ºbaseline!
            group_advantages = [
                (r - group_mean) / group_std
                for r in group_rewards
            ]
            advantages.extend(group_advantages)

        # Step 4: Policy update with PPO-style clipping
        for response, adv in zip(flatten(responses), advantages):
            # è®¡ç®—probability ratio
            log_prob_new = policy_model.log_prob(response)
            log_prob_old = response.log_prob  # å­˜å‚¨çš„old log prob

            ratio = torch.exp(log_prob_new - log_prob_old)

            # Clipped objective
            surr1 = ratio * adv
            surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps) * adv
            loss = -torch.min(surr1, surr2)

            # KL divergence penalty
            kl = log_prob_new - ref_model.log_prob(response)
            total_loss = loss + beta * kl

            # Backprop
            total_loss.backward()

        # Update
        optimizer.step()

        return {
            'avg_reward': np.mean(flatten(rewards)),
            'avg_advantage': np.mean(advantages),
            'kl_div': kl.mean().item()
        }
```

#### **TARPO vs æ ‡å‡†GRPOçš„åŒºåˆ«**

| ç‰¹æ€§ | æ ‡å‡†GRPO | TARPO (Table-R1) | æˆ‘ä»¬çš„æ”¹è¿› |
|------|----------|------------------|-----------|
| **å¥–åŠ±ç»„ä»¶** | å•ä¸€reward | region + answer + consistency | **+ repair quality + efficiency** |
| **åº”ç”¨å¯¹è±¡** | å•æ¬¡ç”Ÿæˆ | å•æ¬¡ç”Ÿæˆ+åŒºåŸŸè¯†åˆ« | **è¿­ä»£ä¿®å¤è¿‡ç¨‹** |
| **è®­ç»ƒç­–ç•¥** | å‡åŒ€é‡‡æ · | Region-aware | **Curriculum learning** |
| **æ•ˆç‡ä¼˜åŒ–** | æ—  | å‡å°‘67.5% token | **åŠ¨æ€è¿­ä»£é¢„ç®—** |

---

### ğŸ” **3. OpenCodeInterpreter (ä»£ç æ‰§è¡Œæ¡†æ¶)**

#### **æ ¸å¿ƒæ¶æ„**

```python
# OpenCodeInterpreterçš„execution feedbackå¾ªç¯

class CodeInterpreter:
    """
    æ ¸å¿ƒ:ä»£ç ç”Ÿæˆ + æ‰§è¡Œ + åé¦ˆå¾ªç¯
    """

    def __init__(self, model):
        self.model = model
        self.executor = CodeExecutor()

    def generate_with_feedback(self, prompt, max_iterations=3):
        code = self.model.generate(prompt)
        history = []

        for iteration in range(max_iterations):
            # Execute code
            result = self.executor.execute(code)

            history.append({
                'code': code,
                'result': result,
                'iteration': iteration
            })

            # Success
            if result.success:
                return code, result

            # Failure - generate feedback
            feedback = self.generate_feedback(result.error, code)

            # Refine code based on feedback
            refinement_prompt = f"""
Previous code:
{code}

Execution error:
{result.error}

Feedback:
{feedback}

Generate corrected code:
"""
            code = self.model.generate(refinement_prompt)

        return code, result

    def generate_feedback(self, error, code):
        """
        ç”Ÿæˆç»“æ„åŒ–åé¦ˆ (æˆ‘ä»¬å¯ä»¥å¢å¼ºè¿™é‡Œ!)
        """
        # åŸºç¡€ç‰ˆæœ¬:ç®€å•error message
        feedback = f"Error: {str(error)}"

        # æˆ‘ä»¬çš„å¢å¼ºç‰ˆæœ¬:åˆ†å±‚è¯Šæ–­
        # feedback = self.error_diagnoser.diagnose(error, code)

        return feedback
```

#### **å¯å¤ç”¨ç»„ä»¶**

1. **ä»£ç æ‰§è¡Œå™¨**
   ```python
   class CodeExecutor:
       def execute(self, code, timeout=30):
           # Sandbox execution
           # Exception handling
           # Result extraction
   ```

2. **åé¦ˆç”Ÿæˆå™¨**
   ```python
   class FeedbackGenerator:
       def generate(self, error, code):
           # Error analysis
           # Suggestion generation
   ```

3. **è¿­ä»£æ§åˆ¶å™¨**
   ```python
   class IterationController:
       def should_continue(self, history):
           # Decide whether to continue iteration
   ```

**æˆ‘ä»¬ç›´æ¥å€Ÿé‰´**: æ‰§è¡Œå¼•æ“ + åŸºç¡€è¿­ä»£æ¡†æ¶
**æˆ‘ä»¬å¢å¼º**: åé¦ˆç”Ÿæˆå™¨ (4å±‚è¯Šæ–­) + GRPOæ§åˆ¶

---

### ğŸ” **4. Chain-of-Table (è¡¨æ ¼æ“ä½œ)**

#### **æ“ä½œå®šä¹‰**

```python
# Chain-of-Tableçš„5ä¸ªæ ¸å¿ƒæ“ä½œ

class TableOperations:
    """
    é¢„å®šä¹‰çš„è¡¨æ ¼æ“ä½œ
    """

    @staticmethod
    def f_select_row(table, row_indices):
        """
        é€‰æ‹©ç‰¹å®šè¡Œ
        Example: f_select_row(table, [1, 3, 5])
        """
        return table.iloc[row_indices]

    @staticmethod
    def f_select_column(table, column_names):
        """
        é€‰æ‹©ç‰¹å®šåˆ—
        Example: f_select_column(table, ['Name', 'Age'])
        """
        return table[column_names]

    @staticmethod
    def f_add_column(table, column_name, values):
        """
        æ·»åŠ æ–°åˆ—
        Example: f_add_column(table, 'GDP_per_capita', gdp/population)
        """
        table[column_name] = values
        return table

    @staticmethod
    def f_group_by(table, column_name):
        """
        åˆ†ç»„ç»Ÿè®¡
        Example: f_group_by(table, 'Country')
        Returns: DataFrame with counts
        """
        grouped = table.groupby(column_name).size().reset_index(name='Count')
        return grouped

    @staticmethod
    def f_sort_by(table, column_name, ascending=True):
        """
        æ’åº
        Example: f_sort_by(table, 'GDP', ascending=False)
        """
        return table.sort_values(by=column_name, ascending=ascending)
```

#### **Dynamic Planningæœºåˆ¶**

```python
class DynamicPlanner:
    """
    Chain-of-Tableçš„æ ¸å¿ƒ:åŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªæ“ä½œ
    """

    def __init__(self, llm):
        self.llm = llm
        self.operation_pool = [
            'f_select_row',
            'f_select_column',
            'f_add_column',
            'f_group_by',
            'f_sort_by'
        ]

    def plan_next_operation(self, table, question, operation_history):
        """
        é€‰æ‹©ä¸‹ä¸€ä¸ªæ“ä½œ
        """
        prompt = f"""
Current table:
{table.to_markdown()}

Question: {question}

Previous operations: {operation_history}

Available operations: {self.operation_pool}

Choose the next operation:
"""

        next_op = self.llm.generate(prompt)
        return next_op

    def execute_operation_chain(self, table, question):
        """
        æ‰§è¡Œå®Œæ•´çš„operation chain
        """
        operation_history = []
        current_table = table.copy()

        while True:
            # Plan next operation
            next_op = self.plan_next_operation(
                current_table, question, operation_history
            )

            if next_op == '[END]':
                break

            # Execute operation
            current_table = self.execute(current_table, next_op)
            operation_history.append(next_op)

        return current_table, operation_history
```

#### **æˆ‘ä»¬çš„é­”æ”¹æ–¹æ¡ˆ**

```python
class EnhancedTableOperations(TableOperations):
    """
    æ‰©å±•Chain-of-Tableæ“ä½œ,åŠ å…¥é”™è¯¯æ¢å¤
    """

    @staticmethod
    def f_rollback(table, operation_history, steps=1):
        """
        ğŸ†• å›æ»šæ“ä½œ (Chain-of-Tableæ²¡æœ‰!)
        """
        # å›é€€åˆ°stepsæ­¥ä¹‹å‰çš„çŠ¶æ€
        pass

    @staticmethod
    def f_python_code(table, code_snippet):
        """
        ğŸ†• æ‰§è¡Œçµæ´»çš„Pythonä»£ç  (çªç ´å›ºå®šæ“ä½œé™åˆ¶)
        """
        namespace = {'df': table.copy(), 'pd': pd, 'np': np}
        exec(code_snippet, namespace)
        return namespace['df']

    @staticmethod
    def f_fuzzy_match_column(table, column_pattern):
        """
        ğŸ†• æ¨¡ç³ŠåŒ¹é…åˆ—å (å¤„ç†åˆ—åé”™è¯¯)
        """
        from difflib import get_close_matches
        matches = get_close_matches(column_pattern, table.columns)
        return matches[0] if matches else None

    @staticmethod
    def f_data_cleaning(table, column_name):
        """
        ğŸ†• æ•°æ®æ¸…æ´— (å¤„ç†ç©ºå€¼ã€ç±»å‹é”™è¯¯)
        """
        # å¡«å……ç©ºå€¼
        table[column_name] = table[column_name].fillna(method='ffill')
        # ç±»å‹è½¬æ¢
        table[column_name] = pd.to_numeric(table[column_name], errors='coerce')
        return table
```

---

## äºŒã€æˆ‘ä»¬çš„å…·ä½“å®ç°æ–¹æ¡ˆ

### ğŸ¯ **Step 1: åŸºç¡€æ¡†æ¶ (Week 1-2)**

#### **1.1 å¤ç°AILS-NTUA Baseline**

```python
# ails_ntua_baseline.py

class AILS_NTUA_Baseline:
    """
    AILS-NTUAçš„åŸºç¡€å®ç°
    """

    def __init__(self, model_name="gpt-4"):
        self.llm = OpenAI(model=model_name)
        self.executor = CodeExecutor()  # å€Ÿç”¨OpenCodeInterpreter
        self.max_iterations = 2  # AILS-NTUAå›ºå®š2æ¬¡

    def answer(self, table, question):
        # Step 1: Task Decomposition
        subtasks = self.decompose_task(table, question)

        # Step 2: Generate Python code
        code = self.generate_code(table, subtasks)

        # Step 3: Execute with error fixing
        for iteration in range(self.max_iterations):
            result = self.executor.execute(code, table)

            if result.success:
                return result.answer

            # Error fixing (ç®€å•ç‰ˆæœ¬)
            code = self.fix_code(code, result.error)

        return None

    def decompose_task(self, table, question):
        """ç®€å•çš„ä»»åŠ¡åˆ†è§£"""
        prompt = f"""
Decompose the question into subtasks:
Question: {question}
Table columns: {table.columns.tolist()}

Subtasks (3-5 steps):
"""
        subtasks = self.llm.generate(prompt)
        return subtasks

    def generate_code(self, table, subtasks):
        """ç”ŸæˆPythonä»£ç """
        prompt = f"""
Generate Python code to solve:
Table: {table.head().to_markdown()}
Subtasks: {subtasks}

Code:
```python
"""
        code = self.llm.generate(prompt)
        return extract_code(code)

    def fix_code(self, code, error):
        """ç®€å•çš„é”™è¯¯ä¿®å¤ (AILS-NTUAé£æ ¼)"""
        prompt = f"""
Fix the error:
Code: {code}
Error: {error}

Corrected code:
```python
"""
        fixed_code = self.llm.generate(prompt)
        return extract_code(fixed_code)
```

**æµ‹è¯•ç›®æ ‡**: WikiTQ ~65% (ä¸AILS-NTUAè®ºæ–‡ä¸€è‡´)

---

### ğŸ¯ **Step 2: æ™ºèƒ½é”™è¯¯è¯Šæ–­ (Week 3-4)**

#### **2.1 4å±‚è¯Šæ–­ç³»ç»Ÿå®ç°**

```python
# intelligent_diagnoser.py

class IntelligentErrorDiagnoser:
    """
    æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°:4å±‚é”™è¯¯è¯Šæ–­
    """

    def __init__(self):
        self.error_classifier = ErrorClassifier()
        self.root_cause_analyzer = RootCauseAnalyzer()
        self.strategy_selector = StrategySelector()
        self.prompt_generator = RepairPromptGenerator()

    def diagnose(self, error, code, table, question):
        """å®Œæ•´è¯Šæ–­æµç¨‹"""
        # Level 1: Classification
        error_type = self.error_classifier.classify(error)

        # Level 2: Root Cause Analysis
        root_cause = self.root_cause_analyzer.analyze(
            error, error_type, code, table
        )

        # Level 3: Strategy Selection
        strategy = self.strategy_selector.select(root_cause)

        # Level 4: Prompt Generation
        repair_prompt = self.prompt_generator.generate(
            error, root_cause, strategy, code, table, question
        )

        return DiagnosisResult(
            error_type=error_type,
            root_cause=root_cause,
            strategy=strategy,
            repair_prompt=repair_prompt
        )


class ErrorClassifier:
    """Level 1: é”™è¯¯åˆ†ç±»"""

    def classify(self, error):
        error_map = {
            'KeyError': ErrorType.MISSING_COLUMN,
            'TypeError': ErrorType.TYPE_MISMATCH,
            'ValueError': ErrorType.INVALID_VALUE,
            'IndexError': ErrorType.INDEX_ERROR,
            'SyntaxError': ErrorType.SYNTAX_ERROR,
            'AttributeError': ErrorType.ATTRIBUTE_ERROR,
        }

        error_name = type(error).__name__
        return error_map.get(error_name, ErrorType.UNKNOWN)


class RootCauseAnalyzer:
    """Level 2: æ ¹å› åˆ†æ"""

    def analyze(self, error, error_type, code, table):
        if error_type == ErrorType.MISSING_COLUMN:
            return self.analyze_missing_column(error, table)
        elif error_type == ErrorType.TYPE_MISMATCH:
            return self.analyze_type_mismatch(error, code, table)
        # ... å…¶ä»–ç±»å‹

    def analyze_missing_column(self, error, table):
        """åˆ†æç¼ºå¤±åˆ—é”™è¯¯"""
        # æå–é”™è¯¯ä¸­çš„åˆ—å
        missing_col = extract_column_from_error(error)

        # Fuzzy matching
        from difflib import get_close_matches
        suggestions = get_close_matches(
            missing_col,
            table.columns.tolist(),
            n=3,
            cutoff=0.6
        )

        return RootCause(
            type='missing_column',
            missing_column=missing_col,
            available_columns=table.columns.tolist(),
            suggestions=suggestions
        )


class StrategySelector:
    """Level 3: ç­–ç•¥é€‰æ‹©"""

    def __init__(self):
        self.strategies = {
            'missing_column': ColumnNameCorrectionStrategy(),
            'type_mismatch': TypeConversionStrategy(),
            'invalid_value': ValueValidationStrategy(),
            'index_error': BoundaryCheckStrategy(),
            # ... 20+ç­–ç•¥
        }

    def select(self, root_cause):
        strategy_map = {
            'missing_column': 'missing_column',
            'type_mismatch': 'type_mismatch',
            'invalid_value': 'invalid_value',
        }

        strategy_id = strategy_map.get(root_cause.type, 'generic')
        return self.strategies[strategy_id]


class RepairPromptGenerator:
    """Level 4: Promptç”Ÿæˆ"""

    def generate(self, error, root_cause, strategy, code, table, question):
        templates = {
            'missing_column': self.missing_column_template,
            'type_mismatch': self.type_mismatch_template,
            # ... å…¶ä»–æ¨¡æ¿
        }

        template = templates.get(root_cause.type, self.generic_template)
        return template(error, root_cause, strategy, code, table, question)

    def missing_column_template(self, error, root_cause, strategy, code, table, question):
        return f"""
## Error Analysis

**Error Type**: Missing Column
**Root Cause**: Column '{root_cause.missing_column}' does not exist

**Available Columns**: {root_cause.available_columns}
**Suggested Matches**: {root_cause.suggestions}

## Repair Strategy

Strategy: {strategy.name}
Recommended Action:
1. Replace '{root_cause.missing_column}' with '{root_cause.suggestions[0]}'
2. Or use case-insensitive matching
3. Or check if column needs to be created

## Previous Code

```python
{code}
```

## Your Task

Generate corrected code that fixes the KeyError.
Use column '{root_cause.suggestions[0]}' instead of '{root_cause.missing_column}'.

Corrected Code:
```python
"""
```

**æµ‹è¯•ç›®æ ‡**: é”™è¯¯ä¿®å¤æˆåŠŸç‡ä»50% â†’ 80%+

---

### ğŸ¯ **Step 3: GRPOå®ç° (Week 7-9)**

#### **3.1 GRPO Trainerå®ç°**

```python
# grpo_trainer.py

class GRPOTrainer:
    """
    åŸºäºDeepSeek GRPOå®ç°
    """

    def __init__(
        self,
        policy_model,
        ref_model,
        group_size=4,
        clip_eps=0.2,
        kl_coef=0.01
    ):
        self.policy_model = policy_model
        self.ref_model = ref_model
        self.group_size = group_size
        self.clip_eps = clip_eps
        self.kl_coef = kl_coef

    def train_step(self, batch_data):
        """
        å•æ­¥GRPOè®­ç»ƒ
        """
        all_trajectories = []
        all_rewards = []

        # å¯¹æ¯ä¸ªæ ·æœ¬ç”Ÿæˆgroup_sizeä¸ªtrajectory
        for sample in batch_data:
            group_trajectories = []
            group_rewards = []

            for _ in range(self.group_size):
                # æ‰§è¡Œè¿­ä»£ä¿®å¤
                result = self.system.iterative_repair(
                    sample.table,
                    sample.question,
                    sample.gold_answer
                )

                # è®¡ç®—trajectory reward
                reward = self.compute_trajectory_reward(
                    result['trajectory'],
                    sample.gold_answer
                )

                group_trajectories.append(result['trajectory'])
                group_rewards.append(reward)

            all_trajectories.extend(group_trajectories)
            all_rewards.append(group_rewards)

        # Group-based advantage estimation
        advantages = self.compute_group_advantages(all_rewards)

        # Policy update
        loss = self.compute_policy_loss(all_trajectories, advantages)
        loss.backward()
        self.optimizer.step()

        return {
            'loss': loss.item(),
            'avg_reward': np.mean(flatten(all_rewards)),
            'avg_advantage': np.mean(advantages)
        }

    def compute_group_advantages(self, all_rewards):
        """
        Group-based advantage (GRPOæ ¸å¿ƒ)
        """
        advantages = []

        for group_rewards in all_rewards:
            # ç”¨group meanä½œä¸ºbaseline
            group_mean = np.mean(group_rewards)
            group_std = np.std(group_rewards) + 1e-8

            # Normalize
            group_adv = [
                (r - group_mean) / group_std
                for r in group_rewards
            ]

            advantages.extend(group_adv)

        return advantages

    def compute_trajectory_reward(self, trajectory, gold_answer):
        """
        è®¡ç®—trajectoryçš„æ€»reward
        """
        final_result = trajectory[-1]['result']

        # Component 1: Execution
        r_exec = 1.0 if final_result.success else -0.5

        # Component 2: Accuracy
        if final_result.success:
            r_acc = compute_accuracy(final_result.answer, gold_answer)
        else:
            r_acc = 0.0

        # Component 3: Efficiency
        num_iter = len(trajectory)
        r_eff = 1.0 / num_iter

        # Component 4: Repair Quality (æˆ‘ä»¬çš„åˆ›æ–°!)
        r_repair = 0.0
        for i in range(1, len(trajectory)):
            prev = trajectory[i-1]['result']
            curr = trajectory[i]['result']

            if not prev.success and curr.success:
                r_repair += 0.5  # æˆåŠŸä¿®å¤
            elif not prev.success and not curr.success:
                if is_improving(prev.error, curr.error):
                    r_repair += 0.2  # é”™è¯¯åœ¨æ”¹å–„

        # Component 5: Code Quality
        final_code = trajectory[-1]['code']
        r_quality = evaluate_code_quality(final_code)

        # Weighted sum
        total_reward = (
            0.3 * r_exec +
            0.4 * r_acc +
            0.1 * r_eff +
            0.1 * r_repair +
            0.1 * r_quality
        )

        return total_reward
```

---

## ä¸‰ã€ä»£ç ä»“åº“ç»“æ„

```
table-qa-grpo/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ wikitq/
â”‚   â”œâ”€â”€ tabfact/
â”‚   â””â”€â”€ fetaqa/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ baselines/
â”‚   â”‚   â”œâ”€â”€ ails_ntua_baseline.py      # AILS-NTUAå¤ç°
â”‚   â”‚   â”œâ”€â”€ chain_of_table.py          # Chain-of-Tableå¤ç°
â”‚   â”‚   â””â”€â”€ direct_qa.py               # Direct QA baseline
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ code_generator.py          # ä»£ç ç”Ÿæˆå™¨
â”‚   â”‚   â”œâ”€â”€ code_executor.py           # æ‰§è¡Œå¼•æ“ (å€Ÿç”¨OpenCodeInterpreter)
â”‚   â”‚   â”œâ”€â”€ table_operations.py        # è¡¨æ ¼æ“ä½œ (å€Ÿç”¨Chain-of-Table)
â”‚   â”‚   â””â”€â”€ iteration_controller.py    # è¿­ä»£æ§åˆ¶
â”‚   â”‚
â”‚   â”œâ”€â”€ diagnosis/
â”‚   â”‚   â”œâ”€â”€ error_classifier.py        # Level 1: åˆ†ç±»
â”‚   â”‚   â”œâ”€â”€ root_cause_analyzer.py     # Level 2: æ ¹å› åˆ†æ
â”‚   â”‚   â”œâ”€â”€ strategy_selector.py       # Level 3: ç­–ç•¥é€‰æ‹©
â”‚   â”‚   â”œâ”€â”€ prompt_generator.py        # Level 4: Promptç”Ÿæˆ
â”‚   â”‚   â””â”€â”€ strategies/                # 20+ä¿®å¤ç­–ç•¥
â”‚   â”‚       â”œâ”€â”€ column_name_correction.py
â”‚   â”‚       â”œâ”€â”€ type_conversion.py
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ grpo/
â”‚   â”‚   â”œâ”€â”€ grpo_trainer.py           # GRPOè®­ç»ƒå™¨
â”‚   â”‚   â”œâ”€â”€ reward_function.py        # Rewardè®¡ç®—
â”‚   â”‚   â””â”€â”€ curriculum_learning.py    # è¯¾ç¨‹å­¦ä¹ 
â”‚   â”‚
â”‚   â””â”€â”€ system/
â”‚       â”œâ”€â”€ hybrid_qa_system.py       # å®Œæ•´ç³»ç»Ÿ
â”‚       â””â”€â”€ config.py                 # é…ç½®
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_baseline.py               # è¿è¡Œbaseline
â”‚   â”œâ”€â”€ run_ablation.py               # æ¶ˆèå®éªŒ
â”‚   â”œâ”€â”€ run_grpo_training.py          # GRPOè®­ç»ƒ
â”‚   â””â”€â”€ evaluate.py                   # è¯„ä¼°è„šæœ¬
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_baseline_analysis.ipynb
â”‚   â””â”€â”€ 03_error_pattern_analysis.ipynb
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_diagnoser.py
    â”œâ”€â”€ test_executor.py
    â””â”€â”€ test_grpo.py
```

---

## å››ã€12å‘¨è¯¦ç»†å®æ–½è®¡åˆ’

### **Week 1: ç¯å¢ƒæ­å»º + æ•°æ®å‡†å¤‡**
```bash
# Day 1-2: ç¯å¢ƒé…ç½®
conda create -n table-qa python=3.10
conda activate table-qa
pip install torch transformers openai pandas scikit-learn

# Day 3-4: æ•°æ®ä¸‹è½½
python scripts/download_wikitq.py
python scripts/download_tabfact.py
python scripts/download_fetaqa.py

# Day 5-7: æ•°æ®é¢„å¤„ç†
python scripts/preprocess_data.py --dataset wikitq
python scripts/preprocess_data.py --dataset tabfact
```

### **Week 2: Baselineå¤ç°**
```bash
# AILS-NTUA Baseline
python experiments/run_baseline.py --method ails_ntua --dataset wikitq
# ç›®æ ‡: ~65% accuracy

# Direct QA Baseline
python experiments/run_baseline.py --method direct_qa --dataset wikitq
# ç›®æ ‡: ~60% accuracy

# Chain-of-Table Baseline
python experiments/run_baseline.py --method chain_of_table --dataset wikitq
# ç›®æ ‡: ~67% accuracy
```

### **Week 3-4: é”™è¯¯è¯Šæ–­ç³»ç»Ÿ**
```python
# å®ç°é¡ºåº
1. ErrorClassifier (Day 1-2)
2. RootCauseAnalyzer (Day 3-5)
3. 10ä¸ªæ ¸å¿ƒç­–ç•¥ (Day 6-10)
4. PromptGenerator (Day 11-12)
5. æµ‹è¯•è¯Šæ–­å‡†ç¡®ç‡ (Day 13-14)

# æµ‹è¯•
python tests/test_diagnoser.py
# ç›®æ ‡: è¯Šæ–­å‡†ç¡®ç‡>85%
```

### **Week 5-6: æ··åˆæ¨ç†æ¡†æ¶**
```python
# é›†æˆChain-of-Tableæ“ä½œ
1. TableOperationsæ‰©å±• (Day 1-3)
2. HybridCodeGeneratorå®ç° (Day 4-7)
3. æ•´åˆæµ‹è¯• (Day 8-10)
4. å¯¹æ¯”å®éªŒ (Day 11-14)

# è¯„ä¼°
python experiments/run_ablation.py --component hybrid_reasoning
# ç›®æ ‡: WikiTQ 68.8% (+1.5% vs baseline)
```

### **Week 7-9: GRPOå®ç°**
```python
# Week 7: GRPO TraineråŸºç¡€
1. Trajectoryè®°å½• (Day 1-2)
2. Rewardè®¡ç®— (Day 3-4)
3. Group advantage (Day 5-7)

# Week 8: Policy update
1. Policy loss (Day 1-3)
2. KL divergence (Day 4-5)
3. Optimizeré…ç½® (Day 6-7)

# Week 9: Curriculum Learning
1. éš¾åº¦åˆ†çº§ (Day 1-2)
2. è®­ç»ƒå¾ªç¯ (Day 3-5)
3. å®Œæ•´è®­ç»ƒ (Day 6-7)

# è®­ç»ƒ
python experiments/run_grpo_training.py --epochs 5
# ç›®æ ‡: WikiTQ 71.2% (+2.4% vs no-GRPO)
```

### **Week 10: å®Œæ•´å®éªŒ**
```bash
# 4ä¸ªæ•°æ®é›†å…¨é¢è¯„ä¼°
python evaluate.py --dataset all --method all

# æ¶ˆèå®éªŒ
python experiments/run_ablation.py --all-components

# é”™è¯¯åˆ†æ
python analysis/error_pattern_mining.py
```

### **Week 11-12: è®ºæ–‡æ’°å†™**
```
Day 1-3: Introduction + Related Work
Day 4-7: Method (3é¡µ)
Day 8-10: Experiments (2é¡µ)
Day 11-12: Analysis + Conclusion
Day 13-14: å›¾è¡¨åˆ¶ä½œ + æ ¡å¯¹
```

---

## äº”ã€å…³é”®ä»£ç ç¤ºä¾‹

### **å®Œæ•´ç³»ç»Ÿä½¿ç”¨**

```python
# main.py

from src.system import HybridTableQASystem
from src.grpo import GRPOTrainer
from data import load_wikitq

# åˆå§‹åŒ–ç³»ç»Ÿ
system = HybridTableQASystem(
    model_name="gpt-4",
    use_grpo=True,
    use_diagnosis=True,    # ä½¿ç”¨æ™ºèƒ½è¯Šæ–­
    use_hybrid=True,       # ä½¿ç”¨æ··åˆæ¨ç†
    max_iterations=5
)

# åŠ è½½æ•°æ®
train_data = load_wikitq(split='train')
val_data = load_wikitq(split='dev')
test_data = load_wikitq(split='test')

# GRPOè®­ç»ƒ
trainer = GRPOTrainer(system)
trainer.train(
    train_data=train_data,
    val_data=val_data,
    num_epochs=5,
    batch_size=16
)

# è¯„ä¼°
accuracy = system.evaluate(test_data)
print(f"Test Accuracy: {accuracy:.2%}")

# å•ä¸ªæ ·æœ¬æ¨ç†
table = test_data[0]['table']
question = test_data[0]['question']

result = system.answer(table, question)
print(f"Answer: {result['answer']}")
print(f"Iterations: {result['iterations']}")
print(f"Success: {result['success']}")
```

---

## å…­ã€æ€»ç»“

### âœ… **æˆ‘ä»¬åšçš„æ˜¯ä»€ä¹ˆ?**

1. **åŸºç¡€**: AILS-NTUA (Language-to-Code + ç®€å•Error Fixing)
2. **å¢å¼º1**: 4å±‚æ™ºèƒ½é”™è¯¯è¯Šæ–­ (vs ç®€å•error message)
3. **å¢å¼º2**: æ··åˆæ¨ç† (Chain-of-Tableæ“ä½œ + çµæ´»ä»£ç )
4. **å¢å¼º3**: GRPOä¼˜åŒ–è¿­ä»£ç­–ç•¥ (vs å›ºå®šç­–ç•¥)
5. **å¢å¼º4**: åŠ¨æ€è¿­ä»£é¢„ç®— (vs å›ºå®š2æ¬¡)

### ğŸ¯ **é¢„æœŸæˆæœ**

- **WikiTQ**: 71.2% (SOTA 67.3%, **+3.9%**)
- **TabFact**: 88.5% (SOTA 86.6%, **+1.9%**)
- **æ•ˆç‡**: å¹³å‡1.8æ¬¡è¿­ä»£ (vs 2.0æ¬¡)
- **ä»£ç **: å®Œå…¨å¼€æº,å¯å¤ç°

### ğŸš€ **ç«‹å³å¼€å§‹!**

```bash
git clone https://github.com/your-username/table-qa-grpo
cd table-qa-grpo
pip install -r requirements.txt
python scripts/setup.py
python experiments/run_baseline.py
```

**å®Œæ•´æ–¹æ¡ˆå·²å‡†å¤‡å°±ç»ª,ç°åœ¨å°±å¼€å§‹å®ç°å§!** ğŸ‰
