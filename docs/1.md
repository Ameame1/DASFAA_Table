 1. GRPO_TableQA_Proposal.md - 完整研究方案

  包含:
  - 研究动机和创新点
  - 详细的系统架构(4个stage的pipeline)
  - 奖励函数设计(多维度,动态权重)
  - 实验设计(4个数据集,9个baseline)
  - 详细的评估指标
  - 预期结果和目标性能
  - 12周实施计划

  2. grpo_table_qa.py - 核心实现代码

  包含:
  - ErrorClassifier: 错误分类器
  - ExecutionEngine: 安全代码执行引擎
  - CodeGenerator: LLM代码生成器(支持迭代修正)
  - RewardFunction: 多组件奖励函数
  - GRPOTrainer: GRPO训练器(group-based advantage)
  - IterativeTableQASystem: 主系统(集成所有组件)

  3. experiments.py - 实验配置和评估脚本

  包含:
  - 完整的实验配置
  - 数据集加载
  - Baseline对比
  - 消融实验
  - 结果可视化(LaTeX表格)

  ---
  🎯 核心创新点

  1. GRPO + 迭代修正
    - 传统方法: 单次生成代码
    - 我们的方法: 最多3次迭代,每次从错误中学习
    - GRPO优化: 使用group-based advantage,无需value function
  2. 多维度奖励函数
  r_total = 0.4·r_exec + 0.4·r_accuracy + 0.1·r_efficiency + 0.1·r_quality
    - 执行成功奖励
    - 答案准确率奖励
    - 效率奖励(越少迭代越好)
    - 代码质量奖励
  3. 智能错误分类
    - Syntax Error → 修复语法
    - Runtime Error → 检查数据类型/列名
    - Logic Error → 重新理解问题
    - 针对性修复策略

  ---
  📊 详细评估指标

  主指标

  | 指标类别   | 具体指标                | 目标值            | 说明          |
  |--------|---------------------|----------------|-------------|
  | 准确率    | Exact Match (EM)    | >70% (WikiTQ)  | 完全匹配准确率     |
  |        | Denotation Accuracy | >71% (WikiTQ)  | 语义等价准确率     |
  |        | F1 Score            | >85%           | 精确率和召回率调和平均 |
  |        | Accuracy            | >88% (TabFact) | 二分类准确率      |
  | 效率     | Avg Iterations      | ≤2.0           | 平均迭代次数      |
  |        | Success@1           | ≥60%           | 首次执行成功率     |
  |        | Success@3           | ≥90%           | 3次内成功率      |
  |        | Avg Time            | ≤5s            | 平均执行时间      |
  | 错误分析   | Syntax Error Rate   | <5%            | 语法错误比例      |
  |        | Runtime Error Rate  | <10%           | 运行时错误比例     |
  |        | Recovery Rate       | >80%           | 错误恢复成功率     |
  | GRPO训练 | Avg Reward          | 监控趋势           | 平均奖励值       |
  |        | KL Divergence       | <0.1           | 策略偏离度       |

  ---
  🏆 Baseline对比

  分级对比体系

  Tier 1: 基础方法

  - Direct QA (GPT-4): 60.5% (WikiTQ)
  - Few-shot CoT: 60.43%

  Tier 2: 代码生成

  - Text-to-SQL: 52.42%
  - Binder: 54.88%
  - Dater: 61.48%

  Tier 3: 表格推理(SOTA)

  - Chain-of-Table: 67.31% ⭐ 当前SOTA
  - TabSQLify: 64.7%
  - AILS-NTUA: ~65% (SemEval 2025冠军)
  - Table-R1: ~68.5% (RL方法)

  Tier 4: 我们的方法

  - Ours-NoIteration: 64.0% (预期)
  - Ours-Iter1: 66.5%
  - Ours-Iter3-NoGRPO: 69.5%
  - Ours-Iter3-GRPO: 71.2% 🎯 目标新SOTA

  ---
  📈 预期结果(详细)

  WikiTQ数据集

  | 方法                    | Accuracy | Δ vs CoT | Avg Iter | Success@1 |
  |-----------------------|----------|----------|----------|-----------|
  | Chain-of-Table (SOTA) | 67.31%   | -        | 3.2      | -         |
  | Ours-NoIteration      | 64.0%    | -3.31%   | 1.0      | 64%       |
  | Ours-Iter1            | 66.5%    | -0.81%   | 1.0      | 58%       |
  | Ours-Iter3-NoGRPO     | 69.5%    | +2.19%   | 1.8      | 62%       |
  | Ours-Iter3-GRPO       | 71.2%    | +3.89%   | 1.8      | 65%       |

  TabFact数据集

  | 方法                    | Accuracy | Δ vs CoT | Avg Iter |
  |-----------------------|----------|----------|----------|
  | Chain-of-Table (SOTA) | 86.61%   | -        | -        |
  | Ours-Iter3-NoGRPO     | 86.8%    | +0.19%   | 1.9      |
  | Ours-Iter3-GRPO       | 88.5%    | +1.89%   | 1.8      |

  效率对比

  | 指标             | Chain-of-Table | AILS-NTUA | Ours-GRPO |
  |----------------|----------------|-----------|-----------|
  | Avg Iterations | 3.2            | 1.6       | 1.8       |
  | Avg Time (s)   | 4.5            | 3.2       | 3.5       |
  | Success@1      | -              | 58%       | 65%       |
  | API Calls      | ~4             | ~2        | ~2.5      |

  ---
  🔬 消融实验设计

  关键问题

  1. 迭代次数的影响?
    - 比较: 0, 1, 3次迭代
    - 预期: 3次最优,但收益递减
  2. GRPO的贡献?
    - 比较: 有GRPO vs 无GRPO
    - 预期: GRPO提升1.5-2%
  3. 奖励函数组件?
    - 比较: 不同权重配置
    - 预期: accuracy和execution权重最关键
  4. 错误类型分析?
    - 哪种错误最常见?
    - 哪种错误最难修复?

  ---
  💻 使用方式

  1. 训练模式

  python experiments.py --mode train --dataset WikiTQ --use_wandb

  2. 评估模式

  python experiments.py --mode eval --dataset all

  3. 消融实验

  python experiments.py --mode ablation --dataset WikiTQ

  ---
  📝 论文贡献

  技术贡献

  1. ✅ 首个将GRPO应用于Table QA迭代修正的工作
  2. ✅ 多维度奖励函数设计
  3. ✅ 错误分类与针对性修复策略

  实验贡献

  1. ✅ 4个标准数据集全面评估
  2. ✅ 与9个SOTA方法系统对比
  3. ✅ 详细消融实验和错误分析

  开源贡献

  1. ✅ 完整代码实现
  2. ✅ GRPO训练检查点
  3. ✅ 错误修复案例数据集

  ---
  📚 核心参考文献

  已整合到方案中:
  1. AILS-NTUA (SemEval 2025) - 你们的主要竞争对手
  2. Table-R1 (2025) - GRPO在Table QA的应用
  3. Chain-of-Table (ICLR 2024) - 表格推理SOTA
  4. DeepSeekMath - GRPO算法原始论文

  ---
  🎯 下一步行动

  1. 数据准备 (1周)
    - 下载WikiTQ, TabFact, FeTaQA
    - 数据预处理和格式转换
  2. Baseline复现 (1周)
    - 实现Direct QA, Few-shot CoT
    - 参考Chain-of-Table官方代码
  3. 系统开发 (3周)
    - 实现迭代修正框架
    - 集成GRPO训练
  4. 实验运行 (2周)
    - 完整评估
    - 消融实验
  5. 论文撰写 (2周)
    - 投稿ACL/EMNLP 2025

  ---
  总结: 这是一个完整的、可执行的研究方案,融合了最新的GRPO技术和迭代错误修正机制。预期在WikiTQ上达到71.2%,超越当前SOTA (67.31%)约4个百分点!
